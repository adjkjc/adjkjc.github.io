
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Brain-reading - Wikipedia</title>
<script>document.documentElement.className="client-js";RLCONF={"wgBreakFrames":!1,"wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgRequestId":"XshSAApAAMEAA2hJ-nwAAACE","wgCSPNonce":!1,"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Brain-reading","wgTitle":"Brain-reading","wgCurRevisionId":956580282,"wgRevisionId":956580282,"wgArticleId":9404689,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 Japanese-language sources (ja)","Use dmy dates from July 2019","All articles with unsourced statements","Articles with unsourced statements from July 2019","All articles with specifically marked weasel-worded phrases","Articles with specifically marked weasel-worded phrases from June 2013",
"Visual perception","Neurotechnology","Computational neuroscience","Emerging technologies"],"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgRelevantPageName":"Brain-reading","wgRelevantArticleId":9404689,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgRedirectedFrom":"Thought_identification","wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgInternalRedirectTargetUrl":"/wiki/Brain-reading","wgWikibaseItemId":"Q4955700","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={
"ext.globalCssJs.user.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","user":"ready","user.options":"loading","ext.cite.styles":"ready","jquery.makeCollapsible.styles":"ready","mediawiki.toc.styles":"ready","skins.vector.styles.legacy":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready"};RLPAGEMODULES=["mediawiki.action.view.redirect","ext.cite.ux-enhancements","site","mediawiki.page.startup","skins.vector.js","mediawiki.page.ready","jquery.makeCollapsible","mediawiki.toc","ext.gadget.ReferenceTooltips","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents",
"ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.cx.eventlogging.campaigns","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp"];</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.options@1hzgi",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});});</script>
<link rel="stylesheet" href="https://en.wikipedia.org/w/load.php?lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cjquery.makeCollapsible.styles%7Cmediawiki.toc.styles%7Cskins.vector.styles.legacy%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="https://en.wikipedia.org/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;raw=1&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="https://en.wikipedia.org/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.35.0-wmf.31"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit"/>
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="https://en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="../../creativecommons.org/licenses/by-sa/3.0/index.html"/>
<link rel="alternate" type="application/atom+xml" title="Wikipedia Atom feed" href="https://en.wikipedia.org/w/index.php?title=Special:RecentChanges&amp;feed=atom"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Brain-reading"/>
<link rel="dns-prefetch" href="https://login.wikimedia.org/"/>
<link rel="dns-prefetch" href="https://meta.wikimedia.org/" />
<!--[if lt IE 9]><script src="/w/resources/lib/html5shiv/html5shiv.js"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Brain-reading rootpage-Brain-reading skin-vector action-view">
<div id="mw-page-base" class="noprint"></div>
<div id="mw-head-base" class="noprint"></div>
<div id="content" class="mw-body" role="main">
	<a id="top"></a>
		<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
	<div class="mw-indicators mw-body-content">
</div>

	<h1 id="firstHeading" class="firstHeading" lang="en">Brain-reading</h1>
	
	<div id="bodyContent" class="mw-body-content">
		<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
		<div id="contentSub"><span class="mw-redirectedfrom">&#160;&#160;(Redirected from <a href="https://en.wikipedia.org/w/index.php?title=Thought_identification&amp;redirect=no" class="mw-redirect" title="Thought identification">Thought identification</a>)</span></div>
		
		
		<div id="jump-to-nav"></div>
		<a class="mw-jump-link" href="Thought_identification.html#mw-head">Jump to navigation</a>
		<a class="mw-jump-link" href="Thought_identification.html#p-search">Jump to search</a>
		<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p>
<b>Brain-reading</b> or <b>thought identification</b> uses the responses of multiple <a href="https://en.wikipedia.org/wiki/Voxel" title="Voxel">voxels</a> in the <a href="https://en.wikipedia.org/wiki/Brain" title="Brain">brain</a> evoked by <a href="https://en.wikipedia.org/wiki/Stimulus_(physiology)" title="Stimulus (physiology)">stimulus</a> then detected by <a href="https://en.wikipedia.org/wiki/FMRI" class="mw-redirect" title="FMRI">fMRI</a> in order to decode the original stimulus. Advances in research have made this possible by using <a href="https://en.wikipedia.org/wiki/Neuroimaging" title="Neuroimaging">human neuroimaging</a> to decode a person's conscious experience based on non-invasive measurements of an individual's brain activity.<sup id="cite_ref-Decoding_mental_states_1-0" class="reference"><a href="Thought_identification.html#cite_note-Decoding_mental_states-1">&#91;1&#93;</a></sup> Brain reading studies differ in the type of decoding (i.e.&#160;classification, identification and reconstruction) employed, the target (i.e.&#160;decoding visual patterns, auditory patterns, <a href="https://en.wikipedia.org/wiki/Cognitive_state" class="mw-redirect" title="Cognitive state">cognitive states</a>), and the decoding algorithms (<a href="https://en.wikipedia.org/wiki/Linear_classification" class="mw-redirect" title="Linear classification">linear classification</a>, <a href="https://en.wikipedia.org/w/index.php?title=Nonlinear_classification&amp;action=edit&amp;redlink=1" class="new" title="Nonlinear classification (page does not exist)">nonlinear classification</a>, <a href="https://en.wikipedia.org/w/index.php?title=Direct_reconstruction&amp;action=edit&amp;redlink=1" class="new" title="Direct reconstruction (page does not exist)">direct reconstruction</a>, <a href="https://en.wikipedia.org/w/index.php?title=Bayesian_reconstruction&amp;action=edit&amp;redlink=1" class="new" title="Bayesian reconstruction (page does not exist)">Bayesian reconstruction</a>,&#160;etc.) employed.
</p><p>Professor of <a href="https://en.wikipedia.org/wiki/Neuropsychology" title="Neuropsychology">neuropsychology</a> Barbara Sahakian qualifies, "A lot of neuroscientists in the field are very cautious and say we can't talk about reading individuals' minds, and right now that is very true, but we're moving ahead so rapidly, it's not going to be that long before we will be able to tell whether someone's making up a story, or whether someone intended to do a crime with a certain degree of certainty."<sup id="cite_ref-Guardian_2-0" class="reference"><a href="Thought_identification.html#cite_note-Guardian-2">&#91;2&#93;</a></sup>
</p>
<div id="toc" class="toc" role="navigation" aria-labelledby="mw-toc-heading"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2 id="mw-toc-heading">Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="Thought_identification.html#Applications"><span class="tocnumber">1</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="Thought_identification.html#Natural_images"><span class="tocnumber">1.1</span> <span class="toctext">Natural images</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="Thought_identification.html#Lie_detector"><span class="tocnumber">1.2</span> <span class="toctext">Lie detector</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="Thought_identification.html#Human-machine_interfaces"><span class="tocnumber">1.3</span> <span class="toctext">Human-machine interfaces</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="Thought_identification.html#Detecting_attention"><span class="tocnumber">1.4</span> <span class="toctext">Detecting attention</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="Thought_identification.html#Detecting_thoughts"><span class="tocnumber">1.5</span> <span class="toctext">Detecting thoughts</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="Thought_identification.html#Detecting_language"><span class="tocnumber">1.6</span> <span class="toctext">Detecting language</span></a></li>
<li class="toclevel-2 tocsection-8"><a href="Thought_identification.html#Predicting_intentions"><span class="tocnumber">1.7</span> <span class="toctext">Predicting intentions</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="Thought_identification.html#Virtual_environments"><span class="tocnumber">1.8</span> <span class="toctext">Virtual environments</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="Thought_identification.html#Emotions"><span class="tocnumber">1.9</span> <span class="toctext">Emotions</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="Thought_identification.html#Security"><span class="tocnumber">1.10</span> <span class="toctext">Security</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="Thought_identification.html#Methods_of_analysis"><span class="tocnumber">2</span> <span class="toctext">Methods of analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-13"><a href="Thought_identification.html#Classification"><span class="tocnumber">2.1</span> <span class="toctext">Classification</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="Thought_identification.html#Reconstruction"><span class="tocnumber">2.2</span> <span class="toctext">Reconstruction</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="Thought_identification.html#EEG"><span class="tocnumber">2.3</span> <span class="toctext">EEG</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="Thought_identification.html#Accuracy"><span class="tocnumber">3</span> <span class="toctext">Accuracy</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="Thought_identification.html#Limitations"><span class="tocnumber">4</span> <span class="toctext">Limitations</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="Thought_identification.html#Ethical_issues"><span class="tocnumber">5</span> <span class="toctext">Ethical issues</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="Thought_identification.html#History"><span class="tocnumber">6</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="Thought_identification.html#Future_research"><span class="tocnumber">7</span> <span class="toctext">Future research</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="Thought_identification.html#See_also"><span class="tocnumber">8</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-22"><a href="Thought_identification.html#References"><span class="tocnumber">9</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-23"><a href="Thought_identification.html#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>
</ul>
</div>

<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=1" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Natural_images">Natural images</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=2" title="Edit section: Natural images">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Identification of complex natural images is possible using voxels from early and <a href="https://en.wikipedia.org/wiki/Extrastriate_cortex" title="Extrastriate cortex">anterior visual cortex areas</a> forward of them (visual areas V3A, V3B, V4, and the lateral occipital) together with <a href="https://en.wikipedia.org/wiki/Bayesian_inference" title="Bayesian inference">Bayesian inference</a>. This brain reading approach uses three components:<sup id="cite_ref-Naselaris_3-0" class="reference"><a href="Thought_identification.html#cite_note-Naselaris-3">&#91;3&#93;</a></sup> a structural encoding model that characterizes responses in early visual areas; a semantic encoding model that characterizes responses in anterior visual areas; and a Bayesian prior that describes the distribution of structural and semantic <a href="https://en.wikipedia.org/wiki/Scene_statistics" title="Scene statistics">scene statistics</a>.<sup id="cite_ref-Naselaris_3-1" class="reference"><a href="Thought_identification.html#cite_note-Naselaris-3">&#91;3&#93;</a></sup>
</p><p>Experimentally the procedure is for subjects to view 1750 <a href="https://en.wikipedia.org/wiki/Black_and_white" title="Black and white">black and white</a> natural images that are correlated with voxel activation in their brains. Then  subjects viewed another 120 novel target images, and information from the earlier scans is used reconstruct them. Natural images used include pictures of a seaside cafe and harbor, performers on a stage, and dense foliage.<sup id="cite_ref-Naselaris_3-2" class="reference"><a href="Thought_identification.html#cite_note-Naselaris-3">&#91;3&#93;</a></sup>
</p><p>In 2008 <a href="https://en.wikipedia.org/wiki/IBM" title="IBM">IBM</a> applied for a patent on how to extract mental images of human faces from the human brain. It uses a feedback loop based on brain measurements of the fusiform gyrus area in the brain which activates proportionate with degree of facial recognition.<sup id="cite_ref-4" class="reference"><a href="Thought_identification.html#cite_note-4">&#91;4&#93;</a></sup>
</p><p>In 2011, a team led by Shinji Nishimoto used only brain recordings to partially reconstruct what volunteers were seeing. The researchers applied a new model, about how moving object information is processed in human brains, while volunteers watched clips from several videos. An algorithm searched through thousands of hours of external YouTube video footage (none of the videos were the same as the ones the volunteers watched) to select the clips that were most similar.<sup id="cite_ref-5" class="reference"><a href="Thought_identification.html#cite_note-5">&#91;5&#93;</a></sup><sup id="cite_ref-6" class="reference"><a href="Thought_identification.html#cite_note-6">&#91;6&#93;</a></sup>  The authors have uploaded demos comparing the watched and the computer-estimated videos.<sup id="cite_ref-7" class="reference"><a href="Thought_identification.html#cite_note-7">&#91;7&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Lie_detector">Lie detector</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=3" title="Edit section: Lie detector">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Brain-reading has been suggested as an alternative to <a href="https://en.wikipedia.org/wiki/Polygraph_machine" class="mw-redirect" title="Polygraph machine">polygraph machines</a> as a form of <a href="https://en.wikipedia.org/wiki/Lie_detection" title="Lie detection">lie detection</a>.<sup id="cite_ref-Wolpe2005_8-0" class="reference"><a href="Thought_identification.html#cite_note-Wolpe2005-8">&#91;8&#93;</a></sup> Another alternative to polygraph machines is <a href="https://en.wikipedia.org/wiki/Blood-oxygen-level_dependent" class="mw-redirect" title="Blood-oxygen-level dependent">blood oxygenated level dependent</a> functional MRI technology (BOLD fMRI). This technique involves the interpretation of the local change in the concentration of oxygenated hemoglobin in the brain, although the relationship between this blood flow and neural activity is not yet completely understood.<sup id="cite_ref-Wolpe2005_8-1" class="reference"><a href="Thought_identification.html#cite_note-Wolpe2005-8">&#91;8&#93;</a></sup>  Another technique to find concealed information is <a href="https://en.wikipedia.org/wiki/Brain_fingerprinting" title="Brain fingerprinting">brain fingerprinting</a>, which uses EEG to ascertain if a person has a specific memory or information by identifying <a href="https://en.wikipedia.org/wiki/P300_(neuroscience)" title="P300 (neuroscience)">P300</a> event related potentials.<sup id="cite_ref-9" class="reference"><a href="Thought_identification.html#cite_note-9">&#91;9&#93;</a></sup>
</p><p>A number of concerns have been raised about the accuracy and ethical implications of brain-reading for this purpose. Laboratory studies have found rates of accuracy of up to 85%; however, there are concerns about what this means for false positive results among non-criminal populations: "If the prevalence of "prevaricators" in the group being examined is low, the test will yield far more false-positive than true-positive results; about one person in five will be incorrectly identified by the test."<sup id="cite_ref-Wolpe2005_8-2" class="reference"><a href="Thought_identification.html#cite_note-Wolpe2005-8">&#91;8&#93;</a></sup>  Ethical problems involved in the use of brain-reading as lie detection include misapplications due to adoption of the technology before its reliability and validity can be properly assessed and due to misunderstanding of the technology, and privacy concerns due to unprecedented access to individual's private thoughts.<sup id="cite_ref-Wolpe2005_8-3" class="reference"><a href="Thought_identification.html#cite_note-Wolpe2005-8">&#91;8&#93;</a></sup> However, it has been noted that the use of polygraph lie detection carries similar concerns about the reliability of the results<sup id="cite_ref-Wolpe2005_8-4" class="reference"><a href="Thought_identification.html#cite_note-Wolpe2005-8">&#91;8&#93;</a></sup>  and violation of privacy.<sup id="cite_ref-10" class="reference"><a href="Thought_identification.html#cite_note-10">&#91;10&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Human-machine_interfaces">Human-machine interfaces</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=4" title="Edit section: Human-machine interfaces">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright"><div class="thumbinner" style="width:142px;"><a href="https://en.wikipedia.org/wiki/File:EPOC_IGN.jpg" class="image"><img alt="" src="https://upload.wikimedia.org/wikipedia/en/thumb/8/83/EPOC_IGN.jpg/140px-EPOC_IGN.jpg" decoding="async" width="140" height="131" class="thumbimage" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/8/83/EPOC_IGN.jpg/210px-EPOC_IGN.jpg 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/8/83/EPOC_IGN.jpg/280px-EPOC_IGN.jpg 2x" data-file-width="288" data-file-height="270" /></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:EPOC_IGN.jpg" class="internal" title="Enlarge"></a></div>The <a href="https://en.wikipedia.org/wiki/Emotiv_Systems#Emotiv_Epoc" title="Emotiv Systems">Emotiv Epoc</a> is one way that users can give commands to devices using only thoughts</div></div></div>
<p>Brain-reading has also been proposed as a method of improving <a href="https://en.wikipedia.org/wiki/Human-machine_interface" class="mw-redirect" title="Human-machine interface">human-machine interfaces</a>, by the use of EEG to detect relevant brain states of a human.<sup id="cite_ref-11" class="reference"><a href="Thought_identification.html#cite_note-11">&#91;11&#93;</a></sup> In recent years, there has been a rapid increase in patents for technology involved in reading brainwaves, rising from fewer than 400 from 2009–2012 to 1600 in 2014.<sup id="cite_ref-12" class="reference"><a href="Thought_identification.html#cite_note-12">&#91;12&#93;</a></sup> These include proposed ways to control video games via brain waves and "<a href="https://en.wikipedia.org/wiki/Neuromarketing" title="Neuromarketing">neuro-marketing</a>" to determine someone's thoughts about a new product or advertisement.
</p><p><a href="https://en.wikipedia.org/wiki/Emotiv_Systems" title="Emotiv Systems">Emotiv Systems</a>, an Australian electronics company, has demonstrated <a href="https://en.wikipedia.org/wiki/Emotiv_Systems#Emotiv_Epoc" title="Emotiv Systems">a headset</a> that can be trained to recognize a user's thought patterns for different commands. Tan Le demonstrated the headset's ability to manipulate virtual objects on screen, and discussed various future applications for such <a href="https://en.wikipedia.org/wiki/Comparison_of_consumer_brain%E2%80%93computer_interfaces" class="mw-redirect" title="Comparison of consumer brain–computer interfaces">brain-computer interface devices</a>, from  powering wheel chairs to replacing the mouse and keyboard.<sup id="cite_ref-TedWaves_13-0" class="reference"><a href="Thought_identification.html#cite_note-TedWaves-13">&#91;13&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Detecting_attention">Detecting attention</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=5" title="Edit section: Detecting attention">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It is possible to track which of two forms of rivalrous binocular illusions a person was subjectively experiencing from fMRI signals.<sup id="cite_ref-14" class="reference"><a href="Thought_identification.html#cite_note-14">&#91;14&#93;</a></sup>
</p><p>When humans think of an object, such as a screwdriver, many different areas of the brain activate. Marcel Just and his colleague, Tom Mitchell, have used fMRI brain scans to teach a computer to identify the various parts of the brain associated with specific thoughts.<sup id="cite_ref-SixtyMinutes_15-0" class="reference"><a href="Thought_identification.html#cite_note-SixtyMinutes-15">&#91;15&#93;</a></sup> This technology also yielded a discovery: similar thoughts in different human brains are surprisingly similar neurologically. To illustrate this, Just and Mitchell used their computer to predict, based on nothing but fMRI data, which of several images a volunteer was thinking about. The computer was 100% accurate, but so far the machine is only distinguishing between 10 images.<sup id="cite_ref-SixtyMinutes_15-1" class="reference"><a href="Thought_identification.html#cite_note-SixtyMinutes-15">&#91;15&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Detecting_thoughts">Detecting thoughts</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=6" title="Edit section: Detecting thoughts">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The category of event which a person freely recalls can be identified from fMRI before they say what they remembered.<sup id="cite_ref-16" class="reference"><a href="Thought_identification.html#cite_note-16">&#91;16&#93;</a></sup>
</p><p>December 16, 2015, a study conducted by Toshimasa Yamazaki at <a href="https://en.wikipedia.org/wiki/Kyushu_Institute_of_Technology" title="Kyushu Institute of Technology">Kyushu Institute of Technology</a> found that during a <a href="https://en.wikipedia.org/wiki/Rock-paper-scissors" class="mw-redirect" title="Rock-paper-scissors">rock-paper-scissors</a> game a computer was able to determine the choice made by the subjects before they moved their hand. An <a href="https://en.wikipedia.org/wiki/Electroencephalography" title="Electroencephalography">EEG</a> was used to measure activity in the <a href="https://en.wikipedia.org/wiki/Broca%27s_area" title="Broca&#39;s area">Broca's area</a> to <i>see</i> the words two seconds before the words were uttered.<sup id="cite_ref-IEICE-Yamazaki1_17-0" class="reference"><a href="Thought_identification.html#cite_note-IEICE-Yamazaki1-17">&#91;17&#93;</a></sup><sup id="cite_ref-discovery-Yamazaki1_18-0" class="reference"><a href="Thought_identification.html#cite_note-discovery-Yamazaki1-18">&#91;18&#93;</a></sup><sup id="cite_ref-Nishinippon-Yamazaki1_19-0" class="reference"><a href="Thought_identification.html#cite_note-Nishinippon-Yamazaki1-19">&#91;19&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Detecting_language">Detecting language</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=7" title="Edit section: Detecting language">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Statistical analysis of <a href="https://en.wikipedia.org/wiki/EEG" class="mw-redirect" title="EEG">EEG</a> brainwaves has been claimed to allow the recognition of <a href="https://en.wikipedia.org/wiki/Phoneme" title="Phoneme">phonemes</a>,<sup id="cite_ref-20" class="reference"><a href="Thought_identification.html#cite_note-20">&#91;20&#93;</a></sup> and at a 60% to 75% level color and visual shape words.<sup id="cite_ref-21" class="reference"><a href="Thought_identification.html#cite_note-21">&#91;21&#93;</a></sup>
</p><p>On 31 January 2012 Brian Pasley and colleagues of University of California Berkeley published their paper in <a href="https://en.wikipedia.org/wiki/PLoS_Biology" class="mw-redirect" title="PLoS Biology">PLoS Biology</a> wherein subjects' internal neural processing of auditory information was decoded and reconstructed as sound on computer by gathering and analyzing electrical signals directly from subjects' brains.<sup id="cite_ref-Pasley_22-0" class="reference"><a href="Thought_identification.html#cite_note-Pasley-22">&#91;22&#93;</a></sup> The research team conducted their studies on the superior temporal gyrus, a region of the brain that is involved in higher order neural processing to make semantic sense from auditory information.<sup id="cite_ref-BBC_auditorydecoding_23-0" class="reference"><a href="Thought_identification.html#cite_note-BBC_auditorydecoding-23">&#91;23&#93;</a></sup> The research team used a computer model to analyze various parts of the brain that might be involved in neural firing while processing auditory signals. Using the computational model, scientists were able to identify the brain activity involved in processing auditory information when subjects were presented with recording of individual words.<sup id="cite_ref-NHS_inner_voice_unlocked_24-0" class="reference"><a href="Thought_identification.html#cite_note-NHS_inner_voice_unlocked-24">&#91;24&#93;</a></sup> Later, the computer model of auditory information processing was used to reconstruct some of the words back into sound based on the neural processing of the subjects. However the reconstructed sounds were not of good quality and could be recognized only when the audio wave patterns of the reconstructed sound were visually matched with the audio wave patterns of the original sound that was presented to the subjects.<sup id="cite_ref-NHS_inner_voice_unlocked_24-1" class="reference"><a href="Thought_identification.html#cite_note-NHS_inner_voice_unlocked-24">&#91;24&#93;</a></sup> However this research marks a direction towards more precise identification of neural activity in cognition.
</p>
<h3><span class="mw-headline" id="Predicting_intentions">Predicting intentions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=8" title="Edit section: Predicting intentions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="https://en.wikipedia.org/wiki/Neuroscience_of_free_will" title="Neuroscience of free will">Neuroscience of free will</a> and <a href="https://en.wikipedia.org/wiki/Neuroscience_of_free_will#Criticisms" title="Neuroscience of free will">Neuroscience of free will §&#160;Criticisms</a></div>
<p>Some researchers in 2008 were able to predict, with 60% accuracy, whether a subject was going to push a button with their left or right hand. This is notable, not just because the accuracy is better than chance, but also because the scientists were able to make these predictions up to 10 seconds before the subject acted – well before the subject felt they had decided.<sup id="cite_ref-25" class="reference"><a href="Thought_identification.html#cite_note-25">&#91;25&#93;</a></sup> This data is even more striking in light of other research suggesting that the decision to move, and possibly the ability to cancel that movement at the last second,<sup id="cite_ref-Kuhn_Brass_2009_26-0" class="reference"><a href="Thought_identification.html#cite_note-Kuhn_Brass_2009-26">&#91;26&#93;</a></sup> may be the results of unconscious processing.<sup id="cite_ref-Matsuhashi,_M._2008_27-0" class="reference"><a href="Thought_identification.html#cite_note-Matsuhashi,_M._2008-27">&#91;27&#93;</a></sup>
</p><p>John Dylan-Haynes has also demonstrated that fMRI can be used to identify whether a volunteer is about to add or subtract two numbers in their head.<sup id="cite_ref-SixtyMinutes_15-2" class="reference"><a href="Thought_identification.html#cite_note-SixtyMinutes-15">&#91;15&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Virtual_environments">Virtual environments</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=9" title="Edit section: Virtual environments">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It has also been shown that brain-reading can be achieved in a complex <a href="https://en.wikipedia.org/wiki/Virtual_environment" title="Virtual environment">virtual environment</a>.<sup id="cite_ref-28" class="reference"><a href="Thought_identification.html#cite_note-28">&#91;28&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Emotions">Emotions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=10" title="Edit section: Emotions">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Just and Mitchell also claim they are beginning to be able to identify kindness, hypocrisy, and love in the brain.<sup id="cite_ref-SixtyMinutes_15-3" class="reference"><a href="Thought_identification.html#cite_note-SixtyMinutes-15">&#91;15&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="Security">Security</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=11" title="Edit section: Security">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In 2013 a project led by University of California Berkeley professor John Chuang published findings on the feasibility of brainwave-based computer authentication as a substitute for passwords. Improvements in the use of biometrics for computer authentication has continually improved since the 1980s, but this research team was looking for a method faster and less intrusive than today's retina scans, fingerprinting, and voice recognition. The technology chosen to improve security measures is an <a href="https://en.wikipedia.org/wiki/Electroencephalogram" class="mw-redirect" title="Electroencephalogram">electroencephalogram</a> (EEG), or brainwave measurer, to improve passwords into "pass thoughts." Using this method Chuang and his team were able to customize tasks and their authentication thresholds to the point where they were able to reduce error rates under 1%, significantly better than other recent methods. In order to better attract users to this new form of security the team is still researching mental tasks that are enjoyable for the user to perform while having their brainwaves identified. In the future this method could be as cheap, accessible, and straightforward as thought itself.<sup id="cite_ref-29" class="reference"><a href="Thought_identification.html#cite_note-29">&#91;29&#93;</a></sup>
</p><p>John-Dylan Haynes states that fMRI can also be used to identify recognition in the brain. He provides the example of a criminal being interrogated about whether he recognizes the scene of the crime or murder weapons.<sup id="cite_ref-SixtyMinutes_15-4" class="reference"><a href="Thought_identification.html#cite_note-SixtyMinutes-15">&#91;15&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Methods_of_analysis">Methods of analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=12" title="Edit section: Methods of analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Classification">Classification</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=13" title="Edit section: Classification">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In classification, a pattern of activity across multiple voxels is used to determine the particular class from which the stimulus was drawn.<sup id="cite_ref-30" class="reference"><a href="Thought_identification.html#cite_note-30">&#91;30&#93;</a></sup> Many studies have classified visual stimuli, but this approach has also been used to classify cognitive states.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="Wikipedia%253ACitation_needed.html" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (July 2019)">citation needed</span></a></i>&#93;</sup>
</p>
<h3><span class="mw-headline" id="Reconstruction">Reconstruction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=14" title="Edit section: Reconstruction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>In reconstruction brain reading the aim is to create a literal picture of the image that was presented. Early studies used voxels from early <a href="https://en.wikipedia.org/wiki/Visual_cortex" title="Visual cortex">visual cortex</a> areas (V1, V2, and V3) to reconstruct geometric stimuli made up of flickering checkerboard patterns.<sup id="cite_ref-31" class="reference"><a href="Thought_identification.html#cite_note-31">&#91;31&#93;</a></sup><sup id="cite_ref-32" class="reference"><a href="Thought_identification.html#cite_note-32">&#91;32&#93;</a></sup>
</p>
<h3><span class="mw-headline" id="EEG">EEG</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=15" title="Edit section: EEG">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>EEG has also been used to identify recognition of specific information or memories by the <a href="https://en.wikipedia.org/wiki/P300_(neuroscience)" title="P300 (neuroscience)">P300</a> event related potential, which has been dubbed '<a href="https://en.wikipedia.org/wiki/Brain_fingerprinting" title="Brain fingerprinting">brain fingerprinting</a>'.<sup id="cite_ref-33" class="reference"><a href="Thought_identification.html#cite_note-33">&#91;33&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Accuracy">Accuracy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=16" title="Edit section: Accuracy">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Brain-reading accuracy is increasing steadily as the quality of the data and the complexity of the decoding algorithms improve. In one recent experiment it was possible to identify which single image was being seen from a set of 120.<sup id="cite_ref-34" class="reference"><a href="Thought_identification.html#cite_note-34">&#91;34&#93;</a></sup> In another it was possible to correctly identify  90% of the time which of two categories the stimulus came and the specific semantic category (out of 23) of the target image 40% of the time.<sup id="cite_ref-Naselaris_3-3" class="reference"><a href="Thought_identification.html#cite_note-Naselaris-3">&#91;3&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Limitations">Limitations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=17" title="Edit section: Limitations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>It has been noted that so far brain-reading is limited. "In practice, exact reconstructions are impossible to achieve by any reconstruction algorithm on the basis of brain activity signals acquired by fMRI. This is because all reconstructions will inevitably be limited by inaccuracies in the encoding models and noise in the measured signals. Our results<sup class="noprint Inline-Template" style="white-space:nowrap;">&#91;<i><a href="Wikipedia%253AManual_of_Style/Words_to_watch.html#Unsupported_attributions" title="Wikipedia:Manual of Style/Words to watch"><span title="The material near this tag possibly uses too-vague attribution or weasel words. (June 2013)">who?</span></a></i>&#93;</sup> demonstrate that the natural image prior is a powerful (if unconventional) tool for mitigating the effects of these fundamental limitations. A natural image prior with only six million images is sufficient to produce reconstructions that are structurally and semantically similar to a target image."<sup id="cite_ref-Naselaris_3-4" class="reference"><a href="Thought_identification.html#cite_note-Naselaris-3">&#91;3&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Ethical_issues">Ethical issues</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=18" title="Edit section: Ethical issues">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface#Ethical_considerations" title="Brain–computer interface">Brain–computer interface §&#160;Ethical considerations</a></div>
<p>With <a href="https://en.wikipedia.org/wiki/Neuroimaging" title="Neuroimaging">brain scanning</a> technology becoming increasingly accurate, experts predict important debates over how and when it should be used. One potential area of application is criminal law. Haynes states that simply refusing to use brain scans on suspects also prevents the wrongly accused from proving their innocence.<sup id="cite_ref-Guardian_2-1" class="reference"><a href="Thought_identification.html#cite_note-Guardian-2">&#91;2&#93;</a></sup> US scholars generally believe that involuntary brain reading, and involuntary polygraph tests, would violate the <a href="https://en.wikipedia.org/wiki/5th_Amendment_to_the_United_States_Constitution" class="mw-redirect" title="5th Amendment to the United States Constitution">5th Amendment's</a> right to not self incriminate.<sup id="cite_ref-35" class="reference"><a href="Thought_identification.html#cite_note-35">&#91;35&#93;</a></sup><sup id="cite_ref-36" class="reference"><a href="Thought_identification.html#cite_note-36">&#91;36&#93;</a></sup> One perspective is to consider whether brain imaging is like testimony, or instead like DNA, blood, or semen. Paul Root Wolpe, director of the Center for Ethics at Emory University in Atlanta predicts that this question will be decided by a Supreme Court case.<sup id="cite_ref-CBS_60_Minutes_37-0" class="reference"><a href="Thought_identification.html#cite_note-CBS_60_Minutes-37">&#91;37&#93;</a></sup>
</p><p>In other countries outside the United States, thought identification has already been used in criminal law. In 2008 an Indian woman was convicted of murder after an EEG of her brain allegedly revealed that she was familiar with the circumstances surrounding the poisoning of her ex-fiancé.<sup id="cite_ref-CBS_60_Minutes_37-1" class="reference"><a href="Thought_identification.html#cite_note-CBS_60_Minutes-37">&#91;37&#93;</a></sup> Some neuroscientists and legal scholars doubt the validity of using thought identification as a whole for anything past research on the nature of deception and the brain.<sup id="cite_ref-Lying_38-0" class="reference"><a href="Thought_identification.html#cite_note-Lying-38">&#91;38&#93;</a></sup>
</p><p><i><a href="https://en.wikipedia.org/wiki/The_Economist" title="The Economist">The Economist</a></i> cautioned people to be "afraid" of the future impact, and some ethicists argue that privacy laws should protect private thoughts. Legal scholar <a href="https://en.wikipedia.org/wiki/Henry_Greely" title="Henry Greely">Hank Greely</a> argues that the court systems could benefit from such technology, and neuroethicist <a href="https://en.wikipedia.org/wiki/Julian_Savulescu" title="Julian Savulescu">Julian Savulescu</a> states that brain data is not fundamentally different from other types of evidence.<sup id="cite_ref-39" class="reference"><a href="Thought_identification.html#cite_note-39">&#91;39&#93;</a></sup> In <i><a href="https://en.wikipedia.org/wiki/Nature_(journal)" title="Nature (journal)">Nature</a></i>, journalist Liam Drew writes about emerging projects to attach brain-reading devices to speech synthesizers or other output devices for the benefit of <a href="https://en.wikipedia.org/wiki/Tetraplegia" title="Tetraplegia">tetraplegics</a>. Such devices could create concerns of accidentally broadcasting the patient's "inner thoughts" rather than merely conscious speech.<sup id="cite_ref-40" class="reference"><a href="Thought_identification.html#cite_note-40">&#91;40&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=19" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright"><div class="thumbinner" style="width:222px;"><a href="https://en.wikipedia.org/wiki/File:MRI-Philips.JPG" class="image"><img alt="" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/MRI-Philips.JPG/220px-MRI-Philips.JPG" decoding="async" width="220" height="201" class="thumbimage" srcset="https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/MRI-Philips.JPG/330px-MRI-Philips.JPG 1.5x, https://upload.wikimedia.org/wikipedia/commons/thumb/e/ee/MRI-Philips.JPG/440px-MRI-Philips.JPG 2x" data-file-width="2254" data-file-height="2056" /></a>  <div class="thumbcaption"><div class="magnify"><a href="https://en.wikipedia.org/wiki/File:MRI-Philips.JPG" class="internal" title="Enlarge"></a></div>MRI scanner that could be used for Thought Identification</div></div></div>
<p>Psychologist <a href="https://en.wikipedia.org/wiki/John-Dylan_Haynes" title="John-Dylan Haynes">John-Dylan Haynes</a> experienced breakthroughs in brain imaging research in 2006 by using <a href="https://en.wikipedia.org/wiki/FMRI" class="mw-redirect" title="FMRI">fMRI</a>. This research included new findings on visual object recognition, tracking dynamic mental processes, <a href="https://en.wikipedia.org/wiki/Lie_detection" title="Lie detection">lie detecting</a>, and decoding unconscious processing. The combination of these four discoveries revealed such a significant amount of information about an individual's thoughts that Haynes termed it "brain reading".<sup id="cite_ref-Decoding_mental_states_1-1" class="reference"><a href="Thought_identification.html#cite_note-Decoding_mental_states-1">&#91;1&#93;</a></sup>
</p><p>The fMRI has allowed research to expand by significant amounts because it can track the activity in an individual's brain by measuring the brain's blood flow. It is currently thought to be the best method for measuring brain activity, which is why it has been used in multiple research experiments in order to improve the understanding of how doctors and psychologists can identify thoughts.<sup id="cite_ref-fMRI_41-0" class="reference"><a href="Thought_identification.html#cite_note-fMRI-41">&#91;41&#93;</a></sup>
</p><p>In a 2020 study, AI using implanted electrodes could correctly transcribe a sentence read aloud from a fifty-sentence test set 97% of the time, given 40 minutes of training data per participant.<sup id="cite_ref-42" class="reference"><a href="Thought_identification.html#cite_note-42">&#91;42&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="Future_research">Future research</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=20" title="Edit section: Future research">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Experts are unsure of how far thought identification can expand, but Marcel Just believed in 2014 that in 3–5 years there will be a machine that is able to read complex thoughts such as 'I hate so-and-so'.<sup id="cite_ref-CBS_60_Minutes_37-2" class="reference"><a href="Thought_identification.html#cite_note-CBS_60_Minutes-37">&#91;37&#93;</a></sup>
</p><p>Donald Marks, founder and chief science officer of MMT, is working on playing back thoughts individuals have after they have already been recorded.<sup id="cite_ref-Mind_Reader_43-0" class="reference"><a href="Thought_identification.html#cite_note-Mind_Reader-43">&#91;43&#93;</a></sup>
</p><p>Researchers at the University of California Berkeley have already been successful in forming, erasing, and reactivating memories in rats. Marks says they are working on applying the same techniques to humans. This discovery could be monumental for war veterans who suffer from <a href="https://en.wikipedia.org/wiki/Posttraumatic_stress_disorder" title="Posttraumatic stress disorder">PTSD</a>.<sup id="cite_ref-Mind_Reader_43-1" class="reference"><a href="Thought_identification.html#cite_note-Mind_Reader-43">&#91;43&#93;</a></sup>
</p><p>Further research is also being done in analyzing brain activity during video games to detect criminals, <a href="https://en.wikipedia.org/wiki/Neuromarketing" title="Neuromarketing">neuromarketing</a>, and using brain scans in government security checks.<sup id="cite_ref-fMRI_41-1" class="reference"><a href="Thought_identification.html#cite_note-fMRI-41">&#91;41&#93;</a></sup><sup id="cite_ref-CBS_60_Minutes_37-3" class="reference"><a href="Thought_identification.html#cite_note-CBS_60_Minutes-37">&#91;37&#93;</a></sup>
</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=21" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a href="https://en.wikipedia.org/wiki/Bayesian_approaches_to_brain_function" title="Bayesian approaches to brain function">Bayesian approaches to brain function</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cyberware" title="Cyberware">Cyberware</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mind_uploading" title="Mind uploading">Mind uploading</a></li>
<li><a href="https://en.wikipedia.org/wiki/Minority_Report_(film)" title="Minority Report (film)"><i>Minority Report</i> (film)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neural_decoding" title="Neural decoding">Neural decoding</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neuroinformatics" title="Neuroinformatics">Neuroinformatics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thoughtcrime" title="Thoughtcrime">Thoughtcrime</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thought_recording_and_reproduction_device" title="Thought recording and reproduction device">Thought recording and reproduction device</a></li></ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=22" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-Decoding_mental_states-1"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-Decoding_mental_states_1-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Decoding_mental_states_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFHaynesGeraint" class="citation web">Haynes, John-Dylan; Geraint, Rees. <a rel="nofollow" class="external text" href="http://www.nature.com/nrn/journal/v7/n7/full/nrn1931.html">"Decoding mental states from brain activity in humans"</a>. <i>Nature Reviews</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Nature+Reviews&amp;rft.atitle=Decoding+mental+states+from+brain+activity+in+humans&amp;rft.aulast=Haynes&amp;rft.aufirst=John-Dylan&amp;rft.au=Geraint%2C+Rees&amp;rft_id=http%3A%2F%2Fwww.nature.com%2Fnrn%2Fjournal%2Fv7%2Fn7%2Ffull%2Fnrn1931.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r951705291">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background-image:url("https://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png");background-image:linear-gradient(transparent,transparent),url("https://upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background-image:url("https://upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("https://upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background-image:url("https://upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png");background-image:linear-gradient(transparent,transparent),url("https://upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg");background-repeat:no-repeat;background-size:9px;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background-image:url("https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png");background-image:linear-gradient(transparent,transparent),url("https://upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg");background-repeat:no-repeat;background-size:12px;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}</style></span>
</li>
<li id="cite_note-Guardian-2"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-Guardian_2-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Guardian_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><i>The Guardian</i>, "<a rel="nofollow" class="external text" href="https://www.theguardian.com/science/2007/feb/09/neuroscience.ethicsofscience">The brain scan that can read people's intentions</a>"</span>
</li>
<li id="cite_note-Naselaris-3"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-Naselaris_3-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Naselaris_3-1"><sup><i><b>b</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Naselaris_3-2"><sup><i><b>c</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Naselaris_3-3"><sup><i><b>d</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Naselaris_3-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFNaselarisPrengerKayOliver2009" class="citation journal">Naselaris, Thomas; Prenger, Ryan J.; Kay, Kendrick N.; Oliver, Michael; Gallant, Jack L. (2009). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5553889">"Bayesian Reconstruction of Natural Images from Human Brain Activity"</a>. <i>Neuron</i>. <b>63</b> (6): 902–15. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neuron.2009.09.006">10.1016/j.neuron.2009.09.006</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5553889">5553889</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/19778517">19778517</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neuron&amp;rft.atitle=Bayesian+Reconstruction+of+Natural+Images+from+Human+Brain+Activity&amp;rft.volume=63&amp;rft.issue=6&amp;rft.pages=902-15&amp;rft.date=2009&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5553889&amp;rft_id=info%3Apmid%2F19778517&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuron.2009.09.006&amp;rft.aulast=Naselaris&amp;rft.aufirst=Thomas&amp;rft.au=Prenger%2C+Ryan+J.&amp;rft.au=Kay%2C+Kendrick+N.&amp;rft.au=Oliver%2C+Michael&amp;rft.au=Gallant%2C+Jack+L.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC5553889&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-4">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://www.freepatentsonline.com/20100049076.pdf">IBM Patent Application: Retrieving mental images of faces from the human brain</a></span>
</li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-5">^</a></b></span> <span class="reference-text"><cite id="CITEREFNishimotoVuNaselarisBenjamini2011" class="citation">Nishimoto, Shinji; Vu, An T.; Naselaris, Thomas; Benjamini, Yuval; <a href="https://en.wikipedia.org/wiki/Bin_Yu" title="Bin Yu">Yu, Bin</a>; Gallant, Jack L. (2011), "Reconstructing Visual Experiences from Brain Activity Evoked by Natural Movies", <i><a href="https://en.wikipedia.org/wiki/Current_Biology" title="Current Biology">Current Biology</a></i>, <b>21</b> (19): 1641–1646, <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cub.2011.08.031">10.1016/j.cub.2011.08.031</a>, <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3326357">3326357</a></span>, <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/21945275">21945275</a></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Current+Biology&amp;rft.atitle=Reconstructing+Visual+Experiences+from+Brain+Activity+Evoked+by+Natural+Movies&amp;rft.volume=21&amp;rft.issue=19&amp;rft.pages=1641-1646&amp;rft.date=2011&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3326357&amp;rft_id=info%3Apmid%2F21945275&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cub.2011.08.031&amp;rft.aulast=Nishimoto&amp;rft.aufirst=Shinji&amp;rft.au=Vu%2C+An+T.&amp;rft.au=Naselaris%2C+Thomas&amp;rft.au=Benjamini%2C+Yuval&amp;rft.au=Yu%2C+Bin&amp;rft.au=Gallant%2C+Jack+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-6">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="http://blogs.scientificamerican.com/observations/2011/09/22/breakthrough-could-enable-others-to-watch-your-dreams-and-memories-video/Scientific">American Blog, Breakthrough Could Enable Others to Watch Your Dreams and Memories &#91;Video&#93;, Philip Yam</a></span>
</li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-7">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=KMA23JJ1M1o">Nishimoto et al. uploaded video, "Nishimoto.etal.2011.3Subjects.mpeg" on Youtube</a></span>
</li>
<li id="cite_note-Wolpe2005-8"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-Wolpe2005_8-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Wolpe2005_8-1"><sup><i><b>b</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Wolpe2005_8-2"><sup><i><b>c</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Wolpe2005_8-3"><sup><i><b>d</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Wolpe2005_8-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFWolpe,_P._R.Foster,_K._R.Langleben,_D._D.2005" class="citation journal">Wolpe, P. R.; Foster, K. R. &amp; Langleben, D. D. (2005). "Emerging neurotechnologies for lie-detection: promises and perils". <i>The American Journal of Bioethics: AJOB</i>. <b>5</b> (2): 39–49. <a href="https://en.wikipedia.org/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.728.9280">10.1.1.728.9280</a></span>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1080%2F15265160590923367">10.1080/15265160590923367</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/16036700">16036700</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+American+Journal+of+Bioethics%3A+AJOB&amp;rft.atitle=Emerging+neurotechnologies+for+lie-detection%3A+promises+and+perils&amp;rft.volume=5&amp;rft.issue=2&amp;rft.pages=39-49&amp;rft.date=2005&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.728.9280&amp;rft_id=info%3Apmid%2F16036700&amp;rft_id=info%3Adoi%2F10.1080%2F15265160590923367&amp;rft.au=Wolpe%2C+P.+R.&amp;rft.au=Foster%2C+K.+R.&amp;rft.au=Langleben%2C+D.+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-9">^</a></b></span> <span class="reference-text"><cite id="CITEREFFarwellRichardsonRichardson2012" class="citation journal">Farwell, Lawrence A.; Richardson, Drew C.; Richardson, Graham M. (5 December 2012). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3713201">"Brain fingerprinting field studies comparing P300-MERMER and P300 brainwave responses in the detection of concealed information"</a>. <i>Cognitive Neurodynamics</i>. <b>7</b> (4): 263–299. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1007%2Fs11571-012-9230-0">10.1007/s11571-012-9230-0</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3713201">3713201</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/23869200">23869200</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Neurodynamics&amp;rft.atitle=Brain+fingerprinting+field+studies+comparing+P300-MERMER+and+P300+brainwave+responses+in+the+detection+of+concealed+information&amp;rft.volume=7&amp;rft.issue=4&amp;rft.pages=263-299&amp;rft.date=2012-12-05&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3713201&amp;rft_id=info%3Apmid%2F23869200&amp;rft_id=info%3Adoi%2F10.1007%2Fs11571-012-9230-0&amp;rft.aulast=Farwell&amp;rft.aufirst=Lawrence+A.&amp;rft.au=Richardson%2C+Drew+C.&amp;rft.au=Richardson%2C+Graham+M.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3713201&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-10">^</a></b></span> <span class="reference-text"><cite id="CITEREFArstila,_V.Scott,_F.2011" class="citation journal">Arstila, V. &amp; Scott, F. (2011). <a rel="nofollow" class="external text" href="http://www.kirj.ee/public/trames_pdf/2011/issue_2/Trames-2011-2-204-212.pdf">"Brain Reading and Mental Privacy"</a> <span class="cs1-format">(PDF)</span>. <i>TRAMES: A Journal of the Humanities &amp; Social Sciences</i>. <b>15</b> (2): 204–212. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.3176%2Ftr.2011.2.08">10.3176/tr.2011.2.08</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=TRAMES%3A+A+Journal+of+the+Humanities+%26+Social+Sciences&amp;rft.atitle=Brain+Reading+and+Mental+Privacy&amp;rft.volume=15&amp;rft.issue=2&amp;rft.pages=204-212&amp;rft.date=2011&amp;rft_id=info%3Adoi%2F10.3176%2Ftr.2011.2.08&amp;rft.au=Arstila%2C+V.&amp;rft.au=Scott%2C+F.&amp;rft_id=http%3A%2F%2Fwww.kirj.ee%2Fpublic%2Ftrames_pdf%2F2011%2Fissue_2%2FTrames-2011-2-204-212.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-11">^</a></b></span> <span class="reference-text"><cite id="CITEREFKirchner,_E._A.Kim,_S._K.Straube,_S.Seeland,_A.2013" class="citation journal">Kirchner, E. A.; Kim, S. K.; Straube, S.; Seeland, A.; Wöhrle, H.; Krell, M. M.; Tabie, M.; Fahle, M. (2013). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864841">"On the Applicability of Brain Reading for Predictive Human-Machine Interfaces in Robotics"</a>. <i>PLOS ONE</i>. <b>8</b> (12): e81732. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1371%2Fjournal.pone.0081732">10.1371/journal.pone.0081732</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3864841">3864841</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/24358125">24358125</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+ONE&amp;rft.atitle=On+the+Applicability+of+Brain+Reading+for+Predictive+Human-Machine+Interfaces+in+Robotics&amp;rft.volume=8&amp;rft.issue=12&amp;rft.pages=e81732&amp;rft.date=2013&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3864841&amp;rft_id=info%3Apmid%2F24358125&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pone.0081732&amp;rft.au=Kirchner%2C+E.+A.&amp;rft.au=Kim%2C+S.+K.&amp;rft.au=Straube%2C+S.&amp;rft.au=Seeland%2C+A.&amp;rft.au=W%C3%B6hrle%2C+H.&amp;rft.au=Krell%2C+M.+M.&amp;rft.au=Tabie%2C+M.&amp;rft.au=Fahle%2C+M.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3864841&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.bbc.com/news/technology-32623063">"Surge in U.S. 'brain-reading' patents"</a>. <i>BBC.com</i>. 7 May 2015.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=BBC.com&amp;rft.atitle=Surge+in+U.S.+%27brain-reading%27+patents&amp;rft.date=2015-05-07&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-32623063&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-TedWaves-13"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-TedWaves_13-0">^</a></b></span> <span class="reference-text">Tan Le: <a rel="nofollow" class="external text" href="http://www.ted.com/talks/tan_le_a_headset_that_reads_your_brainwaves.html">A headset that reads your brainwaves</a></span>
</li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-14">^</a></b></span> <span class="reference-text"><cite id="CITEREFHaynesRees2005" class="citation journal">Haynes, J; Rees, G (2005). "Predicting the Stream of Consciousness from Activity in Human Visual Cortex". <i>Current Biology</i>. <b>15</b> (14): 1301–7. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.cub.2005.06.026">10.1016/j.cub.2005.06.026</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/16051174">16051174</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Current+Biology&amp;rft.atitle=Predicting+the+Stream+of+Consciousness+from+Activity+in+Human+Visual+Cortex&amp;rft.volume=15&amp;rft.issue=14&amp;rft.pages=1301-7&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cub.2005.06.026&amp;rft_id=info%3Apmid%2F16051174&amp;rft.aulast=Haynes&amp;rft.aufirst=J&amp;rft.au=Rees%2C+G&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-SixtyMinutes-15"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-SixtyMinutes_15-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-SixtyMinutes_15-1"><sup><i><b>b</b></i></sup></a> <a href="Thought_identification.html#cite_ref-SixtyMinutes_15-2"><sup><i><b>c</b></i></sup></a> <a href="Thought_identification.html#cite_ref-SixtyMinutes_15-3"><sup><i><b>d</b></i></sup></a> <a href="Thought_identification.html#cite_ref-SixtyMinutes_15-4"><sup><i><b>e</b></i></sup></a></span> <span class="reference-text"><i>60 Minutes</i> "<a rel="nofollow" class="external text" href="http://news.cnet.com/8301-11386_3-10131643-76.html">Technology that can read your mind</a>"</span>
</li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-16">^</a></b></span> <span class="reference-text"><cite id="CITEREFPolynNatuCohenNorman2005" class="citation journal">Polyn, S. M.; Natu, VS; Cohen, JD; Norman, KA (2005). "Category-Specific Cortical Activity Precedes Retrieval During Memory Search". <i>Science</i>. <b>310</b> (5756): 1963–6. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://doi.org/10.1126%2Fscience.1117645">10.1126/science.1117645</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/16373577">16373577</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Category-Specific+Cortical+Activity+Precedes+Retrieval+During+Memory+Search&amp;rft.volume=310&amp;rft.issue=5756&amp;rft.pages=1963-6&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.1117645&amp;rft_id=info%3Apmid%2F16373577&amp;rft.aulast=Polyn&amp;rft.aufirst=S.+M.&amp;rft.au=Natu%2C+VS&amp;rft.au=Cohen%2C+JD&amp;rft.au=Norman%2C+KA&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-IEICE-Yamazaki1-17"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-IEICE-Yamazaki1_17-0">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.ieice.org/ken/paper/20150317SBYj/eng/">"Silent Speech BCI - An investigation for practical problems"</a>. IEICE Technical Committee. 16 December 2015<span class="reference-accessdate">. Retrieved <span class="nowrap">17 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Silent+Speech+BCI+-+An+investigation+for+practical+problems&amp;rft.pub=IEICE+Technical+Committee&amp;rft.date=2015-12-16&amp;rft_id=https%3A%2F%2Fwww.ieice.org%2Fken%2Fpaper%2F20150317SBYj%2Feng%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-discovery-Yamazaki1-18"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-discovery-Yamazaki1_18-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFDanigelis2016" class="citation web">Danigelis, Alyssa (7 January 2016). <a rel="nofollow" class="external text" href="http://news.discovery.com/tech/gear-and-gadgets/mind-reading-computer-knows-what-youre-about-to-say-160107.htm">"Mind-Reading Computer Knows What You're About to Say"</a>. Discovery News<span class="reference-accessdate">. Retrieved <span class="nowrap">17 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Mind-Reading+Computer+Knows+What+You%27re+About+to+Say&amp;rft.pub=Discovery+News&amp;rft.date=2016-01-07&amp;rft.aulast=Danigelis&amp;rft.aufirst=Alyssa&amp;rft_id=http%3A%2F%2Fnews.discovery.com%2Ftech%2Fgear-and-gadgets%2Fmind-reading-computer-knows-what-youre-about-to-say-160107.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-Nishinippon-Yamazaki1-19"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-Nishinippon-Yamazaki1_19-0">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://web.archive.org/web/20160117074934/http://www.nishinippon.co.jp/nnp/national/article/216215">"頭の中の言葉　解読　障害者と意思疎通、ロボット操作も　九工大・山崎教授ら"</a> (in Japanese). <a href="https://en.wikipedia.org/wiki/Nishinippon_Shimbun" title="Nishinippon Shimbun">Nishinippon Shimbun</a>. 4 January 2016. Archived from <a rel="nofollow" class="external text" href="http://www.nishinippon.co.jp/nnp/national/article/216215">the original</a> on 17 January 2016<span class="reference-accessdate">. Retrieved <span class="nowrap">17 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=%E9%A0%AD%E3%81%AE%E4%B8%AD%E3%81%AE%E8%A8%80%E8%91%89%E3%80%80%E8%A7%A3%E8%AA%AD%E3%80%80%E9%9A%9C%E5%AE%B3%E8%80%85%E3%81%A8%E6%84%8F%E6%80%9D%E7%96%8E%E9%80%9A%E3%80%81%E3%83%AD%E3%83%9C%E3%83%83%E3%83%88%E6%93%8D%E4%BD%9C%E3%82%82%E3%80%80%E4%B9%9D%E5%B7%A5%E5%A4%A7%E3%83%BB%E5%B1%B1%E5%B4%8E%E6%95%99%E6%8E%88%E3%82%89&amp;rft.pub=Nishinippon+Shimbun&amp;rft.date=2016-01-04&amp;rft_id=http%3A%2F%2Fwww.nishinippon.co.jp%2Fnnp%2Fnational%2Farticle%2F216215&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-20">^</a></b></span> <span class="reference-text"><cite id="CITEREFSuppesPerreau-GuimaraesWong2009" class="citation journal">Suppes, Patrick; Perreau-Guimaraes, Marcos; Wong, Dik Kin (2009). "Partial Orders of Similarity Differences Invariant Between EEG-Recorded Brain and Perceptual Representations of Language". <i>Neural Computation</i>. <b>21</b> (11): 3228–69. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1162%2Fneco.2009.04-08-764">10.1162/neco.2009.04-08-764</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/19686069">19686069</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=Partial+Orders+of+Similarity+Differences+Invariant+Between+EEG-Recorded+Brain+and+Perceptual+Representations+of+Language&amp;rft.volume=21&amp;rft.issue=11&amp;rft.pages=3228-69&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2009.04-08-764&amp;rft_id=info%3Apmid%2F19686069&amp;rft.aulast=Suppes&amp;rft.aufirst=Patrick&amp;rft.au=Perreau-Guimaraes%2C+Marcos&amp;rft.au=Wong%2C+Dik+Kin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-21">^</a></b></span> <span class="reference-text"><cite id="CITEREFSuppesHanEpelboimLu1999" class="citation journal">Suppes, Patrick; Han, Bing; Epelboim, Julie; Lu, Zhong-Lin (1999). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC24492">"Invariance of brain-wave representations of simple visual images and their names"</a>. <i>Proceedings of the National Academy of Sciences of the United States of America</i>. <b>96</b> (25): 14658–63. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1073%2Fpnas.96.25.14658">10.1073/pnas.96.25.14658</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC24492">24492</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/10588761">10588761</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences+of+the+United+States+of+America&amp;rft.atitle=Invariance+of+brain-wave+representations+of+simple+visual+images+and+their+names&amp;rft.volume=96&amp;rft.issue=25&amp;rft.pages=14658-63&amp;rft.date=1999&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC24492&amp;rft_id=info%3Apmid%2F10588761&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.96.25.14658&amp;rft.aulast=Suppes&amp;rft.aufirst=Patrick&amp;rft.au=Han%2C+Bing&amp;rft.au=Epelboim%2C+Julie&amp;rft.au=Lu%2C+Zhong-Lin&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC24492&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-Pasley-22"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-Pasley_22-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFPasleyDavidMesgaraniFlinker2012" class="citation journal">Pasley, BN; David, SV; Mesgarani, N; Flinker, A; Shamma, SA;  et al. (2012). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3269422">"Reconstructing Speech from Human Auditory Cortex"</a>. <i>PLOS Biol</i>. <b>10</b> (1): e1001251. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1371%2Fjournal.pbio.1001251">10.1371/journal.pbio.1001251</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3269422">3269422</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/22303281">22303281</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLOS+Biol&amp;rft.atitle=Reconstructing+Speech+from+Human+Auditory+Cortex&amp;rft.volume=10&amp;rft.issue=1&amp;rft.pages=e1001251&amp;rft.date=2012&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3269422&amp;rft_id=info%3Apmid%2F22303281&amp;rft_id=info%3Adoi%2F10.1371%2Fjournal.pbio.1001251&amp;rft.aulast=Pasley&amp;rft.aufirst=BN&amp;rft.au=David%2C+SV&amp;rft.au=Mesgarani%2C+N&amp;rft.au=Flinker%2C+A&amp;rft.au=Shamma%2C+SA&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3269422&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-BBC_auditorydecoding-23"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-BBC_auditorydecoding_23-0">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external autonumber" href="https://www.bbc.co.uk/news/science-environment-16811042">[1]</a> Science decodes 'internal voices' BBC News 31 January 2012</span>
</li>
<li id="cite_note-NHS_inner_voice_unlocked-24"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-NHS_inner_voice_unlocked_24-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-NHS_inner_voice_unlocked_24-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><a rel="nofollow" class="external autonumber" href="http://www.nhs.uk/news/2012/02February/Pages/mind-reading-telephathy-inner-voice.aspx">[2]</a> Secrets of the inner voice unlocked 1 Feb 2012</span>
</li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-25">^</a></b></span> <span class="reference-text"><cite id="CITEREFSoonBrassHeinzeHaynes2008" class="citation journal">Soon, C.; Brass, M.; Heinze, H.; Haynes, J. (2008). "Unconscious determinants of free decisions in the human brain". <i>Nature Neuroscience</i>. <b>11</b> (5): 543–545. <a href="https://en.wikipedia.org/wiki/CiteSeerX_(identifier)" class="mw-redirect" title="CiteSeerX (identifier)">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.520.2204">10.1.1.520.2204</a></span>. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fnn.2112">10.1038/nn.2112</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/18408715">18408715</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Neuroscience&amp;rft.atitle=Unconscious+determinants+of+free+decisions+in+the+human+brain&amp;rft.volume=11&amp;rft.issue=5&amp;rft.pages=543-545&amp;rft.date=2008&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.520.2204&amp;rft_id=info%3Apmid%2F18408715&amp;rft_id=info%3Adoi%2F10.1038%2Fnn.2112&amp;rft.aulast=Soon&amp;rft.aufirst=C.&amp;rft.au=Brass%2C+M.&amp;rft.au=Heinze%2C+H.&amp;rft.au=Haynes%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-Kuhn_Brass_2009-26"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-Kuhn_Brass_2009_26-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFKühnBrass2009" class="citation journal">Kühn, S.; Brass, M. (2009). "Retrospective construction of the judgment of free choice". <i>Consciousness and Cognition</i>. <b>18</b> (1): 12–21. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.concog.2008.09.007">10.1016/j.concog.2008.09.007</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/18952468">18952468</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Consciousness+and+Cognition&amp;rft.atitle=Retrospective+construction+of+the+judgment+of+free+choice&amp;rft.volume=18&amp;rft.issue=1&amp;rft.pages=12-21&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1016%2Fj.concog.2008.09.007&amp;rft_id=info%3Apmid%2F18952468&amp;rft.aulast=K%C3%BChn&amp;rft.aufirst=S.&amp;rft.au=Brass%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-Matsuhashi,_M._2008-27"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-Matsuhashi,_M._2008_27-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFMatsuhashiHallett2008" class="citation journal">Matsuhashi, M.; Hallett, M. (2008). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4747633">"The timing of the conscious intention to move"</a>. <i>European Journal of Neuroscience</i>. <b>28</b> (11): 2344–2351. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1111%2Fj.1460-9568.2008.06525.x">10.1111/j.1460-9568.2008.06525.x</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4747633">4747633</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/19046374">19046374</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=European+Journal+of+Neuroscience&amp;rft.atitle=The+timing+of+the+conscious+intention+to+move&amp;rft.volume=28&amp;rft.issue=11&amp;rft.pages=2344-2351&amp;rft.date=2008&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4747633&amp;rft_id=info%3Apmid%2F19046374&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1460-9568.2008.06525.x&amp;rft.aulast=Matsuhashi&amp;rft.aufirst=M.&amp;rft.au=Hallett%2C+M.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC4747633&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-28">^</a></b></span> <span class="reference-text"><cite id="CITEREFChuNiTanSaunders2010" class="citation journal">Chu, Carlton; Ni, Yizhao; Tan, Geoffrey; Saunders, Craig J.; Ashburner, John (2010). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3084459">"Kernel regression for fMRI pattern prediction"</a>. <i>NeuroImage</i>. <b>56</b> (2): 662–673. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neuroimage.2010.03.058">10.1016/j.neuroimage.2010.03.058</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3084459">3084459</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/20348000">20348000</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=Kernel+regression+for+fMRI+pattern+prediction&amp;rft.volume=56&amp;rft.issue=2&amp;rft.pages=662-673&amp;rft.date=2010&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3084459&amp;rft_id=info%3Apmid%2F20348000&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2010.03.058&amp;rft.aulast=Chu&amp;rft.aufirst=Carlton&amp;rft.au=Ni%2C+Yizhao&amp;rft.au=Tan%2C+Geoffrey&amp;rft.au=Saunders%2C+Craig+J.&amp;rft.au=Ashburner%2C+John&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3084459&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-29">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.ischool.berkeley.edu/newsandevents/news/20130403brainwaveauthentication">"NEW RESEARCH: COMPUTERS THAT CAN IDENTIFY YOU BY YOUR THOUGHTS"</a>. <i>UC Berkeley School of Information</i>. UC Berkeley<span class="reference-accessdate">. Retrieved <span class="nowrap">8 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=UC+Berkeley+School+of+Information&amp;rft.atitle=NEW+RESEARCH%3A+COMPUTERS+THAT+CAN+IDENTIFY+YOU+BY+YOUR+THOUGHTS&amp;rft_id=http%3A%2F%2Fwww.ischool.berkeley.edu%2Fnewsandevents%2Fnews%2F20130403brainwaveauthentication&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-30">^</a></b></span> <span class="reference-text"><cite id="CITEREFKamitaniTong2005" class="citation journal"><a href="https://en.wikipedia.org/wiki/Yukiyasu_Kamitani" title="Yukiyasu Kamitani">Kamitani, Yukiyasu</a>; Tong, Frank (2005). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1808230">"Decoding the visual and subjective contents of the human brain"</a>. <i>Nature Neuroscience</i>. <b>8</b> (5): 679–85. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fnn1444">10.1038/nn1444</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1808230">1808230</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/15852014">15852014</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Neuroscience&amp;rft.atitle=Decoding+the+visual+and+subjective+contents+of+the+human+brain&amp;rft.volume=8&amp;rft.issue=5&amp;rft.pages=679-85&amp;rft.date=2005&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1808230&amp;rft_id=info%3Apmid%2F15852014&amp;rft_id=info%3Adoi%2F10.1038%2Fnn1444&amp;rft.aulast=Kamitani&amp;rft.aufirst=Yukiyasu&amp;rft.au=Tong%2C+Frank&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1808230&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-31">^</a></b></span> <span class="reference-text"><cite id="CITEREFMiyawakiUchidaYamashitaSato2008" class="citation journal">Miyawaki, Y; Uchida, H; Yamashita, O; Sato, M; Morito, Y; Tanabe, H; Sadato, N; Kamitani, Y (2008). "Visual Image Reconstruction from Human Brain Activity using a Combination of Multiscale Local Image Decoders". <i>Neuron</i>. <b>60</b> (5): 915–29. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neuron.2008.11.004">10.1016/j.neuron.2008.11.004</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/19081384">19081384</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neuron&amp;rft.atitle=Visual+Image+Reconstruction+from+Human+Brain+Activity+using+a+Combination+of+Multiscale+Local+Image+Decoders&amp;rft.volume=60&amp;rft.issue=5&amp;rft.pages=915-29&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuron.2008.11.004&amp;rft_id=info%3Apmid%2F19081384&amp;rft.aulast=Miyawaki&amp;rft.aufirst=Y&amp;rft.au=Uchida%2C+H&amp;rft.au=Yamashita%2C+O&amp;rft.au=Sato%2C+M&amp;rft.au=Morito%2C+Y&amp;rft.au=Tanabe%2C+H&amp;rft.au=Sadato%2C+N&amp;rft.au=Kamitani%2C+Y&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-32">^</a></b></span> <span class="reference-text"><cite id="CITEREFThirionDuchesnayHubbardDubois2006" class="citation journal">Thirion, Bertrand; Duchesnay, Edouard; Hubbard, Edward; Dubois, Jessica; Poline, Jean-Baptiste; Lebihan, Denis; Dehaene, Stanislas (2006). "Inverse retinotopy: Inferring the visual content of images from brain activation patterns". <i>NeuroImage</i>. <b>33</b> (4): 1104–16. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1016%2Fj.neuroimage.2006.06.062">10.1016/j.neuroimage.2006.06.062</a>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/17029988">17029988</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=Inverse+retinotopy%3A+Inferring+the+visual+content+of+images+from+brain+activation+patterns&amp;rft.volume=33&amp;rft.issue=4&amp;rft.pages=1104-16&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2006.06.062&amp;rft_id=info%3Apmid%2F17029988&amp;rft.aulast=Thirion&amp;rft.aufirst=Bertrand&amp;rft.au=Duchesnay%2C+Edouard&amp;rft.au=Hubbard%2C+Edward&amp;rft.au=Dubois%2C+Jessica&amp;rft.au=Poline%2C+Jean-Baptiste&amp;rft.au=Lebihan%2C+Denis&amp;rft.au=Dehaene%2C+Stanislas&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-33">^</a></b></span> <span class="reference-text">Farwell, Lawrence A., Drew C. Richardson, and Graham M. Richardson. 2012. "Brain Fingerprinting Field Studies Comparing P300-MERMER and P300 Brainwave Responses in the Detection of Concealed Information." <i>Cognitive Neurodynamics</i> 7(4):263–99. Retrieved (https://link.springer.com/article/10.1007/s11571-012-9230-0).</span>
</li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-34">^</a></b></span> <span class="reference-text"><cite id="CITEREFKayNaselarisPrengerGallant2008" class="citation journal">Kay, Kendrick N.; Naselaris, Thomas; Prenger, Ryan J.; Gallant, Jack L. (2008). <a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556484">"Identifying natural images from human brain activity"</a>. <i>Nature</i>. <b>452</b> (7185): 352–5. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fnature06713">10.1038/nature06713</a>. <a href="https://en.wikipedia.org/wiki/PMC_(identifier)" class="mw-redirect" title="PMC (identifier)">PMC</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3556484">3556484</a></span>. <a href="https://en.wikipedia.org/wiki/PMID_(identifier)" class="mw-redirect" title="PMID (identifier)">PMID</a>&#160;<a rel="nofollow" class="external text" href="https://pubmed.ncbi.nlm.nih.gov/18322462">18322462</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=Identifying+natural+images+from+human+brain+activity&amp;rft.volume=452&amp;rft.issue=7185&amp;rft.pages=352-5&amp;rft.date=2008&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3556484&amp;rft_id=info%3Apmid%2F18322462&amp;rft_id=info%3Adoi%2F10.1038%2Fnature06713&amp;rft.aulast=Kay&amp;rft.aufirst=Kendrick+N.&amp;rft.au=Naselaris%2C+Thomas&amp;rft.au=Prenger%2C+Ryan+J.&amp;rft.au=Gallant%2C+Jack+L.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3556484&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-35">^</a></b></span> <span class="reference-text">Allen, Ronald J., and M. Kristin Mace. "The Self-Incrimination Clause Explained and Its Future Predicted." The Journal of Criminal Law and Criminology (1973-) 94, no. 2 (2004): 243-294.</span>
</li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-36">^</a></b></span> <span class="reference-text">Brennan-Marquez, Kiel. "A modest defense of mind reading." Yale JL &amp; Tech. 15 (2012): 214. "Ronald Allen and Kristen Mace discern 'universal agreement' that the
(Mind Reader Machine) is unacceptable."</span>
</li>
<li id="cite_note-CBS_60_Minutes-37"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-CBS_60_Minutes_37-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-CBS_60_Minutes_37-1"><sup><i><b>b</b></i></sup></a> <a href="Thought_identification.html#cite_ref-CBS_60_Minutes_37-2"><sup><i><b>c</b></i></sup></a> <a href="Thought_identification.html#cite_ref-CBS_60_Minutes_37-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.cbsnews.com/news/how-technology-may-soon-read-your-mind/">"How Technology May Soon "Read" Your Mind"</a>. <i>CBS News</i>. CBS<span class="reference-accessdate">. Retrieved <span class="nowrap">8 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=CBS+News&amp;rft.atitle=How+Technology+May+Soon+%22Read%22+Your+Mind&amp;rft_id=http%3A%2F%2Fwww.cbsnews.com%2Fnews%2Fhow-technology-may-soon-read-your-mind%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-Lying-38"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-Lying_38-0">^</a></b></span> <span class="reference-text"><cite id="CITEREFStix" class="citation web">Stix, Gary. <a rel="nofollow" class="external text" href="http://www.scientificamerican.com/article/new-lie-detector/?page=1">"Can fMRI Really Tell If You're Lying?"</a>. <i>Scientific American</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Scientific+American&amp;rft.atitle=Can+fMRI+Really+Tell+If+You%27re+Lying%3F&amp;rft.aulast=Stix&amp;rft.aufirst=Gary&amp;rft_id=http%3A%2F%2Fwww.scientificamerican.com%2Farticle%2Fnew-lie-detector%2F%3Fpage%3D1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-39">^</a></b></span> <span class="reference-text"><cite id="CITEREFSmith2013" class="citation journal">Smith, Kerri (24 October 2013). <a rel="nofollow" class="external text" href="https://www.nature.com/news/brain-decoding-reading-minds-1.13989">"Brain decoding: Reading minds"</a>. <i>Nature News</i>. <b>502</b> (7472): 428. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2F502428a">10.1038/502428a</a><span class="reference-accessdate">. Retrieved <span class="nowrap">14 May</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+News&amp;rft.atitle=Brain+decoding%3A+Reading+minds&amp;rft.volume=502&amp;rft.issue=7472&amp;rft.pages=428&amp;rft.date=2013-10-24&amp;rft_id=info%3Adoi%2F10.1038%2F502428a&amp;rft.aulast=Smith&amp;rft.aufirst=Kerri&amp;rft_id=https%3A%2F%2Fwww.nature.com%2Fnews%2Fbrain-decoding-reading-minds-1.13989&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-40">^</a></b></span> <span class="reference-text"><cite id="CITEREFDrew2019" class="citation journal">Drew, Liam (24 July 2019). <a rel="nofollow" class="external text" href="https://www.nature.com/articles/d41586-019-02214-2">"The ethics of brain–computer interfaces"</a>. <i>Nature</i>. <b>571</b>: S19–S21. <a href="https://en.wikipedia.org/wiki/Doi_(identifier)" class="mw-redirect" title="Doi (identifier)">doi</a>:<a rel="nofollow" class="external text" href="https://doi.org/10.1038%2Fd41586-019-02214-2">10.1038/d41586-019-02214-2</a><span class="reference-accessdate">. Retrieved <span class="nowrap">14 May</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=The+ethics+of+brain%E2%80%93computer+interfaces&amp;rft.volume=571&amp;rft.pages=S19-S21&amp;rft.date=2019-07-24&amp;rft_id=info%3Adoi%2F10.1038%2Fd41586-019-02214-2&amp;rft.aulast=Drew&amp;rft.aufirst=Liam&amp;rft_id=https%3A%2F%2Fwww.nature.com%2Farticles%2Fd41586-019-02214-2&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-fMRI-41"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-fMRI_41-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-fMRI_41-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFSaenz" class="citation web">Saenz, Aaron. <a rel="nofollow" class="external text" href="http://singularityhub.com/2010/03/17/fmri-reads-the-images-in-your-brain-we-know-what-youre-looking-at-video/">"fMRI Reads the Images in Your Brain – We Know What You're Looking At"</a>. <i>SingularityHUB</i>. Singularity University<span class="reference-accessdate">. Retrieved <span class="nowrap">8 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=SingularityHUB&amp;rft.atitle=fMRI+Reads+the+Images+in+Your+Brain+%E2%80%93+We+Know+What+You%27re+Looking+At&amp;rft.aulast=Saenz&amp;rft.aufirst=Aaron&amp;rft_id=http%3A%2F%2Fsingularityhub.com%2F2010%2F03%2F17%2Ffmri-reads-the-images-in-your-brain-we-know-what-youre-looking-at-video%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-42"><span class="mw-cite-backlink"><b><a href="Thought_identification.html#cite_ref-42">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="https://www.theguardian.com/science/2020/mar/30/scientists-develop-ai-that-can-turn-brain-activity-into-text">"Scientists develop AI that can turn brain activity into text"</a>. <i>the Guardian</i>. 30 March 2020<span class="reference-accessdate">. Retrieved <span class="nowrap">31 March</span> 2020</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=the+Guardian&amp;rft.atitle=Scientists+develop+AI+that+can+turn+brain+activity+into+text&amp;rft.date=2020-03-30&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fscience%2F2020%2Fmar%2F30%2Fscientists-develop-ai-that-can-turn-brain-activity-into-text&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
<li id="cite_note-Mind_Reader-43"><span class="mw-cite-backlink">^ <a href="Thought_identification.html#cite_ref-Mind_Reader_43-0"><sup><i><b>a</b></i></sup></a> <a href="Thought_identification.html#cite_ref-Mind_Reader_43-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite id="CITEREFCuthbertson2014" class="citation web">Cuthbertson, Anthony (29 August 2014). <a rel="nofollow" class="external text" href="http://www.ibtimes.co.uk/mind-reader-meet-man-who-records-stores-your-thoughts-dreams-memories-1463094">"Mind Reader: Meet The Man Who Records and Stores Your Thoughts, Dreams and Memories"</a>. <i>International Business Times</i><span class="reference-accessdate">. Retrieved <span class="nowrap">8 December</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=International+Business+Times&amp;rft.atitle=Mind+Reader%3A+Meet+The+Man+Who+Records+and+Stores+Your+Thoughts%2C+Dreams+and+Memories&amp;rft.date=2014-08-29&amp;rft.aulast=Cuthbertson&amp;rft.aufirst=Anthony&amp;rft_id=http%3A%2F%2Fwww.ibtimes.co.uk%2Fmind-reader-meet-man-who-records-stores-your-thoughts-dreams-memories-1463094&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ABrain-reading" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r951705291"/></span>
</li>
</ol></div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit&amp;section=23" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul><li><a rel="nofollow" class="external text" href="https://www.newscientist.com/article/mg20427323.500-brain-scanners-can-tell-what-youre-thinking-about.html?full=true">Brain scanners can tell what you're thinking about</a> <a href="https://en.wikipedia.org/wiki/New_Scientist" title="New Scientist">New Scientist</a> article on brain-reading 28 October 2009</li>
<li><a rel="nofollow" class="external text" href="https://web.archive.org/web/20110227150548/http://pbc.lrdc.pitt.edu/?q=2007-home">2007 Pittsburgh Brain Activity Interpretation Competition</a>:Interpreting subject-driven actions and sensory experience in a rigorously characterized virtual world</li></ul>
<div role="navigation" class="navbox" aria-labelledby="Emerging_technologies" style="padding:3px"><table class="nowraplinks hlist mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit"><tbody><tr><th scope="col" class="navbox-title" colspan="2" style="text-align: center;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Emerging_technologies" title="Template:Emerging technologies"><abbr title="View this template" style="text-align: center;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">v</abbr></a></li><li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Emerging_technologies" title="Template talk:Emerging technologies"><abbr title="Discuss this template" style="text-align: center;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">t</abbr></a></li><li class="nv-edit"><a class="external text" href="https://en.wikipedia.org/w/index.php?title=Template:Emerging_technologies&amp;action=edit"><abbr title="Edit this template" style="text-align: center;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;">e</abbr></a></li></ul></div><div id="Emerging_technologies" style="font-size:114%;margin:0 4em"><a href="https://en.wikipedia.org/wiki/Emerging_technologies" title="Emerging technologies">Emerging technologies</a></div></th></tr><tr><th scope="row" class="navbox-group" style="text-align: center;;width:1%">Fields</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em"></div><table class="nowraplinks navbox-subgroup" style="border-spacing:0"><tbody><tr><th scope="row" class="navbox-group" style="width:1%;text-align: center;"><div style="padding:0.1em 0;line-height:1.2em;"><a href="https://en.wikipedia.org/wiki/Information_and_communications_technology" title="Information and communications technology">Information and<br />communications</a></div></th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Ambient_intelligence" title="Ambient intelligence">Ambient intelligence</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Internet_of_things" title="Internet of things">Internet of things</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_intelligence" title="Artificial intelligence">Artificial intelligence</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence" title="Applications of artificial intelligence">Applications of artificial intelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Progress_in_artificial_intelligence" title="Progress in artificial intelligence">Progress in artificial intelligence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_translation" title="Machine translation">Machine translation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mobile_translation" title="Mobile translation">Mobile translation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_vision" title="Machine vision">Machine vision</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semantic_Web" title="Semantic Web">Semantic Web</a></li>
<li><a href="https://en.wikipedia.org/wiki/Speech_recognition" title="Speech recognition">Speech recognition</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Atomtronics" title="Atomtronics">Atomtronics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Carbon_nanotube_field-effect_transistor" title="Carbon nanotube field-effect transistor">Carbon nanotube field-effect transistor</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cybermethodology" title="Cybermethodology">Cybermethodology</a></li>
<li><a href="https://en.wikipedia.org/wiki/Optical_disc#Fourth-generation" title="Optical disc">Fourth-generation optical discs</a>
<ul><li><a href="https://en.wikipedia.org/wiki/3D_optical_data_storage" title="3D optical data storage">3D optical data storage</a></li>
<li><a href="https://en.wikipedia.org/wiki/Holographic_data_storage" title="Holographic data storage">Holographic data storage</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units" title="General-purpose computing on graphics processing units">GPGPU</a></li>
<li>Memory
<ul><li><a href="https://en.wikipedia.org/wiki/Programmable_metallization_cell" title="Programmable metallization cell">CBRAM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ferroelectric_RAM" title="Ferroelectric RAM">FRAM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Millipede_memory" title="Millipede memory">Millipede</a></li>
<li><a href="https://en.wikipedia.org/wiki/Magnetoresistive_random-access_memory" title="Magnetoresistive random-access memory">MRAM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Nano-RAM" title="Nano-RAM">NRAM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Phase-change_memory" title="Phase-change memory">PRAM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Racetrack_memory" title="Racetrack memory">Racetrack memory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Resistive_random-access_memory" title="Resistive random-access memory">RRAM</a></li>
<li><a href="https://en.wikipedia.org/wiki/SONOS" title="SONOS">SONOS</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Optical_computing" title="Optical computing">Optical computing</a></li>
<li><a href="https://en.wikipedia.org/wiki/Radio-frequency_identification" title="Radio-frequency identification">RFID</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Chipless_RFID" title="Chipless RFID">Chipless RFID</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Software-defined_radio" title="Software-defined radio">Software-defined radio</a></li>
<li><a href="https://en.wikipedia.org/wiki/Three-dimensional_integrated_circuit" title="Three-dimensional integrated circuit">Three-dimensional integrated circuit</a></li></ul>
</div></td></tr><tr><th scope="row" class="navbox-group" style="width:1%;text-align: center;"><a href="https://en.wikipedia.org/wiki/Neuroscience" title="Neuroscience">Neuroscience</a></th><td class="navbox-list navbox-even" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Artificial_brain" title="Artificial brain">Artificial brain</a></li>
<li><a href="https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface" title="Brain–computer interface">Brain–computer interface</a></li>
<li><a href="https://en.wikipedia.org/wiki/Electroencephalography" title="Electroencephalography">Electroencephalography</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mind_uploading" title="Mind uploading">Mind uploading</a>
<ul><li><a class="mw-selflink selflink">Brain-reading</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neuroinformatics" title="Neuroinformatics">Neuroinformatics</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Neuroprosthetics" title="Neuroprosthetics">Neuroprosthetics</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Visual_prosthesis" title="Visual prosthesis">Bionic eye</a></li>
<li><a href="https://en.wikipedia.org/wiki/Brain_implant" title="Brain implant">Brain implant</a></li>
<li><a href="https://en.wikipedia.org/wiki/Exocortex" class="mw-redirect" title="Exocortex">Exocortex</a></li>
<li><a href="https://en.wikipedia.org/wiki/Retinal_implant" title="Retinal implant">Retinal implant</a></li></ul></li></ul>
</div></td></tr></tbody></table><div></div></td></tr><tr><th scope="row" class="navbox-group" style="text-align: center;;width:1%">Topics</th><td class="navbox-list navbox-odd" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px"><div style="padding:0em 0.25em">
<ul><li><a href="https://en.wikipedia.org/wiki/Collingridge_dilemma" title="Collingridge dilemma">Collingridge dilemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Differential_technological_development" title="Differential technological development">Differential technological development</a></li>
<li><a href="https://en.wikipedia.org/wiki/Disruptive_innovation" title="Disruptive innovation">Disruptive innovation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ephemeralization" title="Ephemeralization">Ephemeralization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ethics_of_technology" title="Ethics of technology">Ethics</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Bioethics" title="Bioethics">Bioethics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cyberethics" title="Cyberethics">Cyberethics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neuroethics" title="Neuroethics">Neuroethics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Robot_ethics" title="Robot ethics">Robot ethics</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Exploratory_engineering" title="Exploratory engineering">Exploratory engineering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Fictional_technology" title="Fictional technology">Fictional technology</a></li>
<li><a href="https://en.wikipedia.org/wiki/Proactionary_principle" title="Proactionary principle">Proactionary principle</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technological_change" title="Technological change">Technological change</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Technological_unemployment" title="Technological unemployment">Technological unemployment</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Technological_convergence" title="Technological convergence">Technological convergence</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technological_evolution" title="Technological evolution">Technological evolution</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technological_paradigm" title="Technological paradigm">Technological paradigm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technology_forecasting" title="Technology forecasting">Technology forecasting</a>
<ul><li><a href="https://en.wikipedia.org/wiki/Accelerating_change" title="Accelerating change">Accelerating change</a></li>
<li><a href="https://en.wikipedia.org/wiki/Moore%27s_law" title="Moore&#39;s law">Moore's law</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technology_scouting" title="Technology scouting">Technology scouting</a></li></ul></li>
<li><a href="https://en.wikipedia.org/wiki/Technology_readiness_level" title="Technology readiness level">Technology readiness level</a></li>
<li><a href="https://en.wikipedia.org/wiki/Technology_roadmap" title="Technology roadmap">Technology roadmap</a></li>
<li><a href="https://en.wikipedia.org/wiki/Transhumanism" title="Transhumanism">Transhumanism</a></li></ul>
</div></td></tr><tr><td class="navbox-abovebelow" colspan="2" style="text-align: center;"><div>
<ul><li><img alt="Category" src="https://upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/16px-Folder_Hexagonal_Icon.svg.png" decoding="async" title="Category" width="16" height="14" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/24px-Folder_Hexagonal_Icon.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/4/48/Folder_Hexagonal_Icon.svg/32px-Folder_Hexagonal_Icon.svg.png 2x" data-file-width="36" data-file-height="31" /> <b><a href="https://en.wikipedia.org/wiki/Category:Emerging_technologies" title="Category:Emerging technologies">Category</a></b></li>
<li><img alt="List-Class article" src="https://upload.wikimedia.org/wikipedia/en/thumb/d/db/Symbol_list_class.svg/16px-Symbol_list_class.svg.png" decoding="async" title="List-Class article" width="16" height="16" srcset="https://upload.wikimedia.org/wikipedia/en/thumb/d/db/Symbol_list_class.svg/23px-Symbol_list_class.svg.png 1.5x, https://upload.wikimedia.org/wikipedia/en/thumb/d/db/Symbol_list_class.svg/31px-Symbol_list_class.svg.png 2x" data-file-width="180" data-file-height="185" /> <b><a href="https://en.wikipedia.org/wiki/List_of_emerging_technologies" title="List of emerging technologies">List</a></b></li></ul>
</div></td></tr></tbody></table></div>
<!-- 
NewPP limit report
Parsed by mw1308
Cached time: 20200515130537
Cache expiry: 2592000
Dynamic content: false
Complications: [vary‐revision‐sha1]
CPU time usage: 0.492 seconds
Real time usage: 0.601 seconds
Preprocessor visited node count: 2531/1000000
Post‐expand include size: 97956/2097152 bytes
Template argument size: 1501/2097152 bytes
Highest expansion depth: 13/40
Expensive parser function count: 3/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 145300/5000000 bytes
Number of Wikibase entities loaded: 0/400
Lua time usage: 0.232/10.000 seconds
Lua memory usage: 5.1 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  516.199      1 -total
 56.73%  292.842      1 Template:Reflist
 21.21%  109.511     10 Template:Cite_web
 21.03%  108.542     20 Template:Cite_journal
 12.07%   62.308      1 Template:Use_dmy_dates
 10.59%   54.648      2 Template:Fix
 10.12%   52.214      1 Template:Citation_needed
  7.87%   40.650      2 Template:Navbox
  7.30%   37.676      1 Template:Emerging_technologies
  6.92%   35.699      3 Template:Category_handler
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:9404689-0!canonical and timestamp 20200515130537 and revision id 956580282
 -->
</div><noscript><img src="https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>
		<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;oldid=956580282">https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;oldid=956580282</a>"</div>
		<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Visual_perception" title="Category:Visual perception">Visual perception</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Neurotechnology" title="Category:Neurotechnology">Neurotechnology</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Computational_neuroscience" title="Category:Computational neuroscience">Computational neuroscience</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Emerging_technologies" title="Category:Emerging technologies">Emerging technologies</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:CS1_Japanese-language_sources_(ja)" title="Category:CS1 Japanese-language sources (ja)">CS1 Japanese-language sources (ja)</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Use_dmy_dates_from_July_2019" title="Category:Use dmy dates from July 2019">Use dmy dates from July 2019</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_July_2019" title="Category:Articles with unsourced statements from July 2019">Articles with unsourced statements from July 2019</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_with_specifically_marked_weasel-worded_phrases" title="Category:All articles with specifically marked weasel-worded phrases">All articles with specifically marked weasel-worded phrases</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_specifically_marked_weasel-worded_phrases_from_June_2013" title="Category:Articles with specifically marked weasel-worded phrases from June 2013">Articles with specifically marked weasel-worded phrases from June 2013</a></li></ul></div></div>
		<div class="visualClear"></div>
		
	</div>
</div>
<div id='mw-data-after-content'>
	<div class="read-more-container"></div>
</div>

<div id="mw-navigation">
	<h2>Navigation menu</h2>
	<div id="mw-head">
		
<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
	<h3 id="p-personal-label">Personal tools</h3>
	<ul >
		<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=Brain-reading" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=Brain-reading" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>
	</ul>
</div>

		<div id="left-navigation">
			<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
	<h3 id="p-namespaces-label">Namespaces</h3>
	<ul >
		<li id="ca-nstab-main" class="selected"><a href="https://en.wikipedia.org/wiki/Brain-reading" title="View the content page [c]" accesskey="c">Article</a></li><li id="ca-talk"><a href="https://en.wikipedia.org/wiki/Talk:Brain-reading" rel="discussion" title="Discuss improvements to the content page [t]" accesskey="t">Talk</a></li>
	</ul>
</div>

			<div id="p-variants" role="navigation" class="emptyPortlet vectorMenu" aria-labelledby="p-variants-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />
	<h3 id="p-variants-label">
		<span>Variants</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>

		</div>
		<div id="right-navigation">
			<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
	<h3 id="p-views-label">Views</h3>
	<ul >
		<li id="ca-view" class="collapsible selected"><a href="https://en.wikipedia.org/wiki/Brain-reading">Read</a></li><li id="ca-edit" class="collapsible"><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></li><li id="ca-history" class="collapsible"><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></li>
	</ul>
</div>

			<div id="p-cactions" role="navigation" class="emptyPortlet vectorMenu" aria-labelledby="p-cactions-label">
	<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />
	<h3 id="p-cactions-label">
		<span>More</span>
	</h3>
	<ul class="menu" >
		
	</ul>
</div>

			<div id="p-search" role="search">
	<h3 >
		<label for="searchInput">Search</label>
	</h3>
	<form action="https://en.wikipedia.org/w/index.php" id="searchform">
		<div id="simpleSearch">
			<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/>
			<input type="hidden" value="Special:Search" name="title"/>
			<input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/>
			<input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>
		</div>
	</form>
</div>

		</div>
	</div>
	
<div id="mw-panel">
	<div id="p-logo" role="banner">
		<a  title="Visit the main page" class="mw-wiki-logo" href="Main_Page.html"></a>
	</div>
	<div class="portal portal-first" role="navigation" id="p-navigation"  aria-labelledby="p-navigation-label">
	<h3  id="p-navigation-label">
		Navigation
	</h3>
	<div class="body">
		<ul><li id="n-mainpage-description"><a href="Main_Page.html" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Wikipedia:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="../../donate.wikimedia.org/wiki/Special%253AFundraiserRedirector@utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en.html" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="https://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li></ul>
		
	</div>
</div>


	<div class="portal" role="navigation" id="p-interaction"  aria-labelledby="p-interaction-label">
	<h3  id="p-interaction-label">
		Interaction
	</h3>
	<div class="body">
		<ul><li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-tb"  aria-labelledby="p-tb-label">
	<h3  id="p-tb-label">
		Tools
	</h3>
	<div class="body">
		<ul><li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/Brain-reading" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Brain-reading" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;oldid=956580282" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4955700" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=Brain-reading&amp;id=956580282&amp;wpFormIdentifier=titleform" title="Information on how to cite this page">Cite this page</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-coll-print_export"  aria-labelledby="p-coll-print_export-label">
	<h3  id="p-coll-print_export-label">
		Print/export
	</h3>
	<div class="body">
		<ul><li id="coll-download-as-rl"><a href="https://en.wikipedia.org/w/index.php?title=Special:ElectronPdf&amp;page=Brain-reading&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=Brain-reading&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li></ul>
		
	</div>
</div>

<div class="portal" role="navigation" id="p-lang"  aria-labelledby="p-lang-label">
	<h3  id="p-lang-label">
		Languages
	</h3>
	<div class="body">
		<ul><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D8%B0%D9%87%D9%86%E2%80%8C%D8%AE%D9%88%D8%A7%D9%86%DB%8C" title="ذهن‌خوانی – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li><li class="interlanguage-link interwiki-pt"><a href="https://pt.wikipedia.org/wiki/Leitura_cerebral" title="Leitura cerebral – Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target">Português</a></li></ul>
		<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4955700#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>
	</div>
</div>


</div>

</div>

<div id="footer" class="mw-footer" role="contentinfo" >
	<ul id="footer-info" >
		<li id="footer-info-lastmod"> This page was last edited on 14 May 2020, at 04:12<span class="anonymous-show">&#160;(UTC)</span>.</li>
		<li id="footer-info-copyright">Text is available under the <a rel="license" href="https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="../../creativecommons.org/licenses/by-sa/3.0/index.html" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="../../wikimediafoundation.org/wiki/Terms_of_Use.html">Terms of Use</a> and <a href="../../wikimediafoundation.org/wiki/Privacy_policy.html">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="https://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
	</ul>
	<ul id="footer-places" >
		<li id="footer-places-privacy"><a href="../../wikimediafoundation.org/wiki/Privacy_policy.html" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
		<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
		<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
		<li id="footer-places-contact"><a href="https://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
		<li id="footer-places-developers"><a href="../../www.mediawiki.org/wiki/Special%253AMyLanguage/How_to_contribute.html">Developers</a></li>
		<li id="footer-places-statslink"><a href="https://stats.wikimedia.org/#/en.wikipedia.org">Statistics</a></li>
		<li id="footer-places-cookiestatement"><a href="../../wikimediafoundation.org/wiki/Cookie_statement.html">Cookie statement</a></li>
		<li id="footer-places-mobileview"><a href="https://en.m.wikipedia.org/w/index.php?title=Brain-reading&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
	</ul>
	<ul id="footer-icons" class="noprint">
		<li id="footer-copyrightico"><a href="../../www.wikimediafoundation.org/index.html"><img src="https://en.wikipedia.org/static/images/wikimedia-button.png" srcset="https://en.wikipedia.org/static/images/wikimedia-button-1.5x.png 1.5x, https://en.wikipedia.org/static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a></li>
		<li id="footer-poweredbyico"><a href="https://www.mediawiki.org/"><img src="https://en.wikipedia.org/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="https://en.wikipedia.org/static/images/poweredby_mediawiki_132x47.png 1.5x, https://en.wikipedia.org/static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a></li>
	</ul>
	<div style="clear: both;"></div>
</div>


<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.492","walltime":"0.601","ppvisitednodes":{"value":2531,"limit":1000000},"postexpandincludesize":{"value":97956,"limit":2097152},"templateargumentsize":{"value":1501,"limit":2097152},"expansiondepth":{"value":13,"limit":40},"expensivefunctioncount":{"value":3,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":145300,"limit":5000000},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  516.199      1 -total"," 56.73%  292.842      1 Template:Reflist"," 21.21%  109.511     10 Template:Cite_web"," 21.03%  108.542     20 Template:Cite_journal"," 12.07%   62.308      1 Template:Use_dmy_dates"," 10.59%   54.648      2 Template:Fix"," 10.12%   52.214      1 Template:Citation_needed","  7.87%   40.650      2 Template:Navbox","  7.30%   37.676      1 Template:Emerging_technologies","  6.92%   35.699      3 Template:Category_handler"]},"scribunto":{"limitreport-timeusage":{"value":"0.232","limit":"10.000"},"limitreport-memusage":{"value":5351421,"limit":52428800}},"cachereport":{"origin":"mw1308","timestamp":"20200515130537","ttl":2592000,"transientcontent":false}}});});</script>
<script type="application/ld+json">{"@context":"https:\/\/schema.org","@type":"Article","name":"Brain-reading","url":"https:\/\/en.wikipedia.org\/wiki\/Brain-reading","sameAs":"http:\/\/www.wikidata.org\/entity\/Q4955700","mainEntity":"http:\/\/www.wikidata.org\/entity\/Q4955700","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\/\/www.wikimedia.org\/static\/images\/wmf-hor-googpub.png"}},"datePublished":"2007-02-09T14:02:34Z","dateModified":"2020-05-14T04:12:07Z"}</script>
<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":93,"wgHostname":"mw1389"});});</script></body></html>
